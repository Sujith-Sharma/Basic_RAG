{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def load_documents(directory_path):\n",
        "    documents=[]\n",
        "    for filename in os.listdir(directory_path):\n",
        "      file_path=os.path.join(directory_path,filename)\n",
        "      with open(file_path, 'r') as file:\n",
        "        text=file.read()\n",
        "        documents.append(text)\n",
        "    return documents"
      ],
      "metadata": {
        "id": "xm90ReHhUtH4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "p7sFiiycUhtf"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text, chunk_size=500):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), chunk_size):\n",
        "        chunk = ' '.join(words[i:i + chunk_size])\n",
        "        chunks.append(chunk)\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "def generate_embeddings(texts, model, tokenizer):\n",
        "    embeddings = []\n",
        "    for text in texts:\n",
        "        inputs = tokenizer(text, return_tensors='pt',\n",
        "                          truncation=True, padding=True)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        embedding = outputs.last_hidden_state.mean(dim=1)\n",
        "        embeddings.append(embedding)\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "Pm4SNRpwL02Z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
        "\n",
        "def retrieve_top_k(query_embedding, vector_db, k=3):\n",
        "    similarities = []\n",
        "    for idx, doc_embedding in enumerate(vector_db['embeddings']):\n",
        "        sim = cosine_similarity(query_embedding, doc_embedding)\n",
        "        similarities.append((idx, sim))\n",
        "\n",
        "    top_k = sorted(similarities, key=lambda x: x[1], reverse=True)[:k]\n",
        "    return [vector_db['texts'][idx] for idx, _ in top_k]"
      ],
      "metadata": {
        "id": "oeW4mkYaL4TB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(query, retrieved_chunks, llm):\n",
        "    context = \"\\n\".join(retrieved_chunks)\n",
        "    prompt = f\"\"\"Based on this context: {context}\n",
        "\n",
        "    Answer the question: {query}\"\"\"\n",
        "\n",
        "    response = llm.generate(prompt)\n",
        "    return response"
      ],
      "metadata": {
        "id": "kQ9yXENiL65z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XMg_S0a2MIoj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}